# AMD_Robotics_Hackathon_2025_Sushi_TaishoğŸ£
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)

**Title:** AMD_RoboticHackathon2025-Sushi-Taisho

**Team:** Cog Bots
- [Mamoru Ota](https://github.com/Mamo1031) (@Mamo1031)
- [Kentaro Fujii](https://github.com/oakwood-fujiken) (@oakwood-fujiken)
- [Yuta Nomura](https://github.com/nomutin) (@nomutin)
- [Tetsugo To](https://github.com/tetsugo02) (@tetsugo02)



## ğŸ¯ Summary
This project simulates a rotary sushi bar ('Kaiten-sushi') using a motorized toy train track. The SO-101 robot arm is tasked with dynamically tracking and picking up sushi samples moving along the rails.

![Sushi-Bot demo](assets/demo.gif)



## âœ¨ Features
<!-- TODO: å®Ÿè£…ã—ãŸæ©Ÿèƒ½ã®è©³ç´°ã‚’è¿½è¨˜ -->
- **??:** ??
- **??:** ??



## ğŸ“¦ Installation
### Prerequisites
<!-- TODO: Prerequisitesã‚’è¿½è¨˜ -->
- Python 3.8+
- [uv](https://github.com/astral-sh/uv) (Python package manager)
- ??


### Setup Instructions
```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh
# Or on Windows: powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Clone the repository
git clone https://github.com/Mamo1031/AMD_Robotics_Hackathon_2025_Sushi_Taisho.git
cd AMD_Robotics_Hackathon_2025_Sushi_Taisho

# Install dependencies and create virtual environment
uv sync

# Activate virtual environment
source .venv/bin/activate  # On Linux/Mac
# On Windows: .venv\Scripts\activate

# Additional setup steps
# TODO: è¿½åŠ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ã‚’è¿½è¨˜
```


### Environment Variables
<!-- TODO: å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’è¿½è¨˜ -->
```bash
# TODO: Environment variables as needed
```



## ğŸ“Š Dataset
<!-- TODO: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ç¢ºå®šã•ã›ãŸã‚‰æ›´æ–° -->
- **Description:**  
  - [TODO: 1ã€œ2æ–‡ã§ã€Œã©ã‚“ãªã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‹ã€ã€Œã©ã†ã„ã†ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã€ã‚’æ›¸ã]
- **Hugging Face URL:**  
  - [TODO: Hugging Faceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®URLã‚’è¿½è¨˜]



## ğŸ¤– Model Training
### Model Architecture
<!-- TODO: ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚’è¿½è¨˜ -->
?? (TODO: ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚’è¿½è¨˜)


### Training Scripts
<!-- TODO: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½¿ç”¨æ–¹æ³•ã‚’è¿½è¨˜ -->
```bash
# Train the model (optional)
uv run train                # uses config/training_config.yaml by default

# Or specify a custom config
uv run train configs/custom_training.yaml
```


### Trained Models
<!-- TODO: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¿½è¨˜ -->
- **Model Information:**  
  - [TODO: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¿½è¨˜]
- **Hugging Face URL:**  
  - [TODO: Hugging Faceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ãƒ¢ãƒ‡ãƒ«ã®URLã‚’è¿½è¨˜]



## ğŸš€ Usage
### Running Inference

<!-- TODO: æ¨è«–ã®å®Ÿè¡Œæ–¹æ³•ã‚’è¿½è¨˜ -->
```bash
# Basic grasping demo (uses config/inference_config.yaml by default)
uv run infer --mode grasp

# VLA demo with natural language instruction (default config)
uv run infer --mode vla --instruction "I want to eat salmon."

# Or specify a custom inference config
uv run infer configs/custom_inference.yaml --mode vla --instruction "I want to eat salmon."
```



## ğŸ¥ Demo Video
<!-- TODO: ãƒ‡ãƒ¢ãƒ“ãƒ‡ã‚ªã®ãƒªãƒ³ã‚¯ã‚’è¿½è¨˜ -->
Link to demo video: [TODO: ãƒ‡ãƒ¢ãƒ“ãƒ‡ã‚ªã®ãƒªãƒ³ã‚¯ã‚’è¿½è¨˜]


## ğŸ“ Project Structure
```
AMD_Robotics_Hackathon_2025_Sushi_Taisho/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â”œâ”€â”€ LICENSE
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ test.gif
â”œâ”€â”€ mission1/
â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â””â”€â”€ [code and script]
â”‚   â””â”€â”€ wandb/
â”‚       â””â”€â”€ [latest run directory copied from wandb of your training job]
â””â”€â”€ mission2/
    â”œâ”€â”€ code/
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ training_config.yaml
    â”‚   â”‚   â””â”€â”€ inference_config.yaml
    â”‚   â”œâ”€â”€ scripts/
    â”‚   â”‚   â”œâ”€â”€ train.py
    â”‚   â”‚   â””â”€â”€ inference.py
    â”‚   â””â”€â”€ src/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ cli.py
    â””â”€â”€ wandb/
        â””â”€â”€ [latest run directory copied from wandb of your training job]
```



## ğŸ”® Future Improvements
<!-- TODO: ä»Šå¾Œã®æ”¹å–„ç‚¹ã‚’è¿½è¨˜ -->
?? (TODO: ä»Šå¾Œã®æ”¹å–„ç‚¹ã‚’è¿½è¨˜)



## ğŸ“„ License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.



## ğŸ™ Acknowledgments
- **AMD and the AMD Open Robotics Hackathon organizers** for providing SO-101 robotics kits, AMD Ryzenâ„¢ AI laptops, and AMD Developer Cloud access with AMD Instinctâ„¢ MI300X GPUs, as well as the event venue and support infrastructure ([event site](https://amdroboticshackathon.datamonsters.com/)).
- **Data Monsters** for operating and coordinating the hackathon program.
- **Hugging Face and the LeRobot team** for releasing the LeRobot framework and examples that this project builds upon.
- **All staff and fellow participants** at the Tokyo venue.


---


**Note:** This repository was created for the AMD Open Robotics Hackathon 2025. All code, models, and documentation are original work developed specifically for this competition.
